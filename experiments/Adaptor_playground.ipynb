{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7-1yLEAp4wEm",
    "outputId": "ced8282c-5b8b-42d4-c4e2-c171f8ef0096"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/gaussalgo/adaptor\n",
      "  Cloning https://github.com/gaussalgo/adaptor to /tmp/pip-req-build-6_0yn149\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/gaussalgo/adaptor /tmp/pip-req-build-6_0yn149\n",
      "  Resolved https://github.com/gaussalgo/adaptor to commit db33e6e439babc68fe801a8946d87116ff44f170\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.7 in /opt/conda/lib/python3.9/site-packages (from adaptor==0.1.4) (1.11.0)\n",
      "Requirement already satisfied: transformers>=4.10.2 in /opt/conda/lib/python3.9/site-packages (from adaptor==0.1.4) (4.18.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.9/site-packages (from adaptor==0.1.4) (0.1.96)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.7->adaptor==0.1.4) (4.2.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.10.2->adaptor==0.1.4) (0.6.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers>=4.10.2->adaptor==0.1.4) (3.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.10.2->adaptor==0.1.4) (2022.4.24)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers>=4.10.2->adaptor==0.1.4) (2.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.10.2->adaptor==0.1.4) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.10.2->adaptor==0.1.4) (1.21.6)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.9/site-packages (from transformers>=4.10.2->adaptor==0.1.4) (0.0.53)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.10.2->adaptor==0.1.4) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.10.2->adaptor==0.1.4) (4.63.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.10.2->adaptor==0.1.4) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->transformers>=4.10.2->adaptor==0.1.4) (3.0.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers>=4.10.2->adaptor==0.1.4) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers>=4.10.2->adaptor==0.1.4) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers>=4.10.2->adaptor==0.1.4) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers>=4.10.2->adaptor==0.1.4) (2.0.12)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from sacremoses->transformers>=4.10.2->adaptor==0.1.4) (1.16.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from sacremoses->transformers>=4.10.2->adaptor==0.1.4) (1.1.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from sacremoses->transformers>=4.10.2->adaptor==0.1.4) (8.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+https://github.com/katarinagresova/AgoBind\n",
      "  Cloning https://github.com/katarinagresova/AgoBind to /tmp/pip-req-build-8k7msi3z\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/katarinagresova/AgoBind /tmp/pip-req-build-8k7msi3z\n",
      "  Resolved https://github.com/katarinagresova/AgoBind to commit ca59766c661cfc253745429111c9d0baeaa0b9d3\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.9/site-packages (from agobind==0.0.1) (2.27.1)\n",
      "Requirement already satisfied: pip>=20.0.1 in /opt/conda/lib/python3.9/site-packages (from agobind==0.0.1) (22.0.4)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.9/site-packages (from agobind==0.0.1) (1.21.6)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.9/site-packages (from agobind==0.0.1) (1.4.2)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in /opt/conda/lib/python3.9/site-packages (from agobind==0.0.1) (4.63.0)\n",
      "Requirement already satisfied: transformers>=4.17.0 in /opt/conda/lib/python3.9/site-packages (from agobind==0.0.1) (4.18.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas>=1.1.4->agobind==0.0.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas>=1.1.4->agobind==0.0.1) (2022.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.23.0->agobind==0.0.1) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.23.0->agobind==0.0.1) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.23.0->agobind==0.0.1) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.23.0->agobind==0.0.1) (2.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.17.0->agobind==0.0.1) (2022.4.24)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.9/site-packages (from transformers>=4.17.0->agobind==0.0.1) (0.0.53)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.17.0->agobind==0.0.1) (0.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.17.0->agobind==0.0.1) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.17.0->agobind==0.0.1) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.17.0->agobind==0.0.1) (0.12.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers>=4.17.0->agobind==0.0.1) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers>=4.17.0->agobind==0.0.1) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->transformers>=4.17.0->agobind==0.0.1) (3.0.8)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas>=1.1.4->agobind==0.0.1) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from sacremoses->transformers>=4.17.0->agobind==0.0.1) (8.1.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from sacremoses->transformers>=4.17.0->agobind==0.0.1) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sklearn in /opt/conda/lib/python3.9/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.9/site-packages (from sklearn) (1.0.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.8.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.21.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: comet-ml in /opt/conda/lib/python3.9/site-packages (3.31.1)\n",
      "Requirement already satisfied: requests-toolbelt>=0.8.0 in /opt/conda/lib/python3.9/site-packages (from comet-ml) (0.9.1)\n",
      "Requirement already satisfied: wrapt>=1.11.2 in /opt/conda/lib/python3.9/site-packages (from comet-ml) (1.14.1)\n",
      "Requirement already satisfied: everett[ini]>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from comet-ml) (3.0.0)\n",
      "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /opt/conda/lib/python3.9/site-packages (from comet-ml) (4.4.0)\n",
      "Requirement already satisfied: dulwich!=0.20.33,>=0.20.6 in /opt/conda/lib/python3.9/site-packages (from comet-ml) (0.20.35)\n",
      "Requirement already satisfied: semantic-version>=2.8.0 in /opt/conda/lib/python3.9/site-packages (from comet-ml) (2.9.0)\n",
      "Requirement already satisfied: requests>=2.18.4 in /opt/conda/lib/python3.9/site-packages (from comet-ml) (2.27.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from comet-ml) (1.16.0)\n",
      "Requirement already satisfied: websocket-client>=0.55.0 in /opt/conda/lib/python3.9/site-packages (from comet-ml) (1.3.2)\n",
      "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /opt/conda/lib/python3.9/site-packages (from comet-ml) (7.352.0)\n",
      "Requirement already satisfied: wurlitzer>=1.0.2 in /opt/conda/lib/python3.9/site-packages (from comet-ml) (3.0.2)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.9/site-packages (from dulwich!=0.20.33,>=0.20.6->comet-ml) (2021.10.8)\n",
      "Requirement already satisfied: urllib3>=1.24.1 in /opt/conda/lib/python3.9/site-packages (from dulwich!=0.20.33,>=0.20.6->comet-ml) (1.26.9)\n",
      "Requirement already satisfied: configobj in /opt/conda/lib/python3.9/site-packages (from everett[ini]>=1.0.1->comet-ml) (5.0.6)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (0.18.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (21.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.18.4->comet-ml) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.18.4->comet-ml) (2.0.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torchmetrics in /opt/conda/lib/python3.9/site-packages (0.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.9/site-packages (from torchmetrics) (1.21.6)\n",
      "Requirement already satisfied: torch>=1.3.1 in /opt/conda/lib/python3.9/site-packages (from torchmetrics) (1.11.0)\n",
      "Requirement already satisfied: pyDeprecate==0.3.* in /opt/conda/lib/python3.9/site-packages (from torchmetrics) (0.3.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from torchmetrics) (21.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.3.1->torchmetrics) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->torchmetrics) (3.0.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install git+https://github.com/gaussalgo/adaptor\n",
    "%pip install git+https://github.com/katarinagresova/AgoBind\n",
    "%pip install sklearn\n",
    "%pip install comet-ml\n",
    "%pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XQoSnzdd40nr"
   },
   "outputs": [],
   "source": [
    "import comet_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "z5x0pz1C43RC"
   },
   "outputs": [],
   "source": [
    "# 1. pick the model base\n",
    "from adaptor.lang_module import LangModule\n",
    "\n",
    "kmer_len = 6\n",
    "stride = 1\n",
    "lang_module = LangModule(f\"armheb/DNA_bert_{kmer_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lpMoF7qo45nc"
   },
   "outputs": [],
   "source": [
    "# 2. Initialize training arguments\n",
    "# We apply NUM_STEPS stopping strategy in cases where at least one of the objectives does not converge in max_steps\n",
    "from adaptor.utils import AdaptationArguments, StoppingStrategy\n",
    "\n",
    "training_arguments = AdaptationArguments(output_dir=\"dnabert_for_clash\",\n",
    "                                         learning_rate=2e-4,\n",
    "                                         weight_decay=0.01,\n",
    "                                         max_steps=100000,\n",
    "                                         stopping_strategy=StoppingStrategy.ALL_OBJECTIVES_CONVERGED,\n",
    "                                         # stopping_strategy=StoppingStrategy.NUM_STEPS_ALL_OBJECTIVES,\n",
    "                                         do_train=True,\n",
    "                                         do_eval=True,\n",
    "                                         warmup_steps=5000,\n",
    "                                         gradient_accumulation_steps=4,\n",
    "                                         logging_steps=100,\n",
    "                                         eval_steps=100,\n",
    "                                         save_steps=100,\n",
    "                                         num_train_epochs=30,\n",
    "                                         evaluation_strategy=\"steps\",\n",
    "                                         also_log_converged_objectives=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FqR2Q1zb47j9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def prepare_data(path_to_csv, path_to_txt, path_to_labels):\n",
    "    dset = pd.read_csv(path_to_csv, sep='\\t')\n",
    "    dset['seq'] = dset.apply(lambda x: x['miRNA'] + 'NNNN' + x['gene'], axis=1)\n",
    "    dset['seq'] = dset['seq'].apply(lambda x: ' '.join([x[i:i+kmer_len] for i in range(0, len(x)-kmer_len+1, stride)]))\n",
    "    np.savetxt(path_to_txt, dset['seq'].values, fmt='%s')\n",
    "    np.savetxt(path_to_labels, dset['label'].values, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-6wBwnXQ5ALn"
   },
   "outputs": [],
   "source": [
    "prepare_data('../data/train_set_1_1_CLASH2013_paper.tsv', '../data/train_set_1_1_CLASH2013_paper.txt', '../data/train_set_1_1_CLASH2013_paper_labels.txt')\n",
    "prepare_data('../data/evaluation_set_1_1_CLASH2013_paper.tsv', '../data/evaluation_set_1_1_CLASH2013_paper.txt', '../data/evaluation_set_1_1_CLASH2013_paper_labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adaptor.evaluators.sequence_classification import SeqClassificationEvaluator, SeqClassificationEvaluator\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "#from torchmetrics import PrecisionRecallCurve\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "from adaptor.evaluators.evaluator_base import EvaluatorBase\n",
    "from adaptor.utils import Head, AdaptationDataset\n",
    "    \n",
    "class SequenceAucPr(SeqClassificationEvaluator):\n",
    "    \"\"\"\n",
    "    Sequence classification area under Precision-Recall curve, where each input sample of dataset falls into a single category.\n",
    "    \"\"\"\n",
    "\n",
    "    smaller_is_better: bool = False\n",
    "\n",
    "    def __call__(self, model: torch.nn.Module, tokenizer: PreTrainedTokenizer, dataset: AdaptationDataset) -> float:\n",
    "        \"\"\"\n",
    "        Refer to the superclass documentation.\n",
    "        \"\"\"\n",
    "        expected = []\n",
    "        actual = []\n",
    "\n",
    "        for batch in dataset:\n",
    "            expected.extend(batch[\"labels\"].cpu())\n",
    "            actual.extend(model(**batch).logits.argmax(-1).cpu())\n",
    "\n",
    "        assert len(expected) == len(actual)\n",
    "\n",
    "        #pr_curve = PrecisionRecallCurve(pos_label=1)\n",
    "        p, r, thresholds = precision_recall_curve(expected, actual)\n",
    "        auc_precision_recall = auc(r, p)\n",
    "        return auc_precision_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QMXvuaJX5WXX",
    "outputId": "04bcc9b4-5467-436c-e5dd-9a0603332802"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at armheb/DNA_bert_6 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at armheb/DNA_bert_6 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "These layers of the loaded SEQ_CLASSIFICATION were not merged: ['weight', 'bias', 'weight', 'bias']\n"
     ]
    }
   ],
   "source": [
    "# 3. pick objectives\n",
    "# Objectives take either List[str] for in-memory iteration, or a source file path for streamed iterati\n",
    "from adaptor.objectives.MLM import MaskedLanguageModeling\n",
    "from adaptor.objectives.classification import SequenceClassification\n",
    "\n",
    "mlm = MaskedLanguageModeling(lang_module,\n",
    "                                 batch_size=16,\n",
    "                                 texts_or_path='../data/train_set_1_1_CLASH2013_paper.txt',\n",
    "                                 val_texts_or_path='../data/evaluation_set_1_1_CLASH2013_paper.txt',\n",
    "                            )\n",
    "\n",
    "cls = SequenceClassification(lang_module,\n",
    "                                  batch_size=64,\n",
    "                                  texts_or_path='../data/train_set_1_1_CLASH2013_paper.txt',\n",
    "                                  labels_or_path='../data/train_set_1_1_CLASH2013_paper_labels.txt',\n",
    "                                 val_texts_or_path='../data/evaluation_set_1_1_CLASH2013_paper.txt',\n",
    "                                 val_labels_or_path='../data/evaluation_set_1_1_CLASH2013_paper_labels.txt',\n",
    "                                 val_evaluators=[SequenceAucPr(decides_convergence=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "DlLlrNy05Ysd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total number of train samples: 30784\n",
      "Total number of eval samples: 2000\n"
     ]
    }
   ],
   "source": [
    "# 4. pick a schedule of the selected objectives\n",
    "# This one will initially fit the first objective until convergence on its eval set, then fits the second one \n",
    "from adaptor.schedules import ParallelSchedule, SequentialSchedule\n",
    "\n",
    "schedule = SequentialSchedule([cls], training_arguments)\n",
    "#schedule = ParallelSchedule([mlm, cls], training_arguments)\n",
    "#schedule = SequentialSchedule(\n",
    "#    objectives=[mlm, cls], \n",
    "#    args=training_arguments\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2JMVI06p5dT2",
    "outputId": "5bea2e2a-532d-44e5-d39f-e6e0f6c9afda"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Comet API key is valid\n",
      "COMET INFO: Comet API key saved in /home/jovyan/.comet.config\n"
     ]
    }
   ],
   "source": [
    "comet_ml.init(project_name='dnabert_for_clash', api_key='3NQhHgMmmlfnoqTcvkG03nYo9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6A1mQYUs5hJr",
    "outputId": "c8ad14a4-b1b8-41a9-965e-acf35841cdfc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 14430\n",
      "  Num Epochs = 28\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 100000\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/katarinagresova/dnabert-for-clash/2beeacbecc194ac0ae1de8b72c521388\n",
      "\n",
      "Automatic Comet.ml online logging enabled\n",
      "Converged objectives: []\n",
      "SequenceClassification:   3%|▎         | 14/481 [00:03<01:34,  4.92batches/s, epoch=1, loss=0.681, split=train]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:  83%|████████▎ | 400/481 [00:49<00:09,  8.57batches/s, epoch=1, loss=0.699, split=train]Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.737, 'learning_rate': 4.000000000000001e-06, 'train_SequenceClassification_loss': 0.73701636813581, 'train_SequenceClassification_num_batches': 400, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:  97%|█████████▋| 30/31 [00:01<00:00, 20.98batches/s, epoch=1, loss=0.701, split=eval]/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f748168acd0> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 21.34batches/s, epoch=1, loss=0.711, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-100/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-100/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-100/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.709191083908081, 'eval_runtime': 1.5023, 'eval_samples_per_second': 20.635, 'eval_steps_per_second': 20.635, 'eval_SequenceClassification_loss': 0.7092440202832222, 'eval_SequenceClassification_num_batches': 32, 'eval_SequenceClassification_SequenceAucPr': 0.5950905306971904, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-100/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:04<00:00,  8.12batches/s, epoch=1, loss=0.416, split=train]Converged objectives: []\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:04<00:00,  7.43batches/s, epoch=1, loss=0.416, split=train]\n",
      "SequenceClassification:  66%|██████▋   | 319/481 [00:37<00:18,  8.53batches/s, epoch=2, loss=1.19, split=train] Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8427, 'learning_rate': 8.000000000000001e-06, 'train_SequenceClassification_loss': 0.7898371073231101, 'train_SequenceClassification_num_batches': 800, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:   0%|          | 0/31 [00:51<?, ?batches/s, epoch=1, loss=-1, split=eval]\n",
      "SequenceClassification: 100%|██████████| 31/31 [00:01<00:00, 20.74batches/s, epoch=2, loss=1.15, split=eval] /opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f74693bd5b0> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 21.13batches/s, epoch=2, loss=1.14, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-200/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-200/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-200/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7621480822563171, 'eval_runtime': 1.5163, 'eval_samples_per_second': 20.444, 'eval_steps_per_second': 20.444, 'eval_SequenceClassification_loss': 0.7416698397137225, 'eval_SequenceClassification_num_batches': 64, 'eval_SequenceClassification_SequenceAucPr': 0.75, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-200/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:02<00:00,  8.34batches/s, epoch=2, loss=0.0118, split=train]Converged objectives: []\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:02<00:00,  7.71batches/s, epoch=2, loss=0.0118, split=train]\n",
      "SequenceClassification:  49%|████▉     | 238/481 [00:28<00:28,  8.56batches/s, epoch=3, loss=0.00343, split=train]Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0458, 'learning_rate': 1.2e-05, 'train_SequenceClassification_loss': 0.9217161289104261, 'train_SequenceClassification_num_batches': 1000, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:   0%|          | 0/31 [00:51<?, ?batches/s, epoch=2, loss=-1, split=eval]\n",
      "SequenceClassification:  97%|█████████▋| 30/31 [00:01<00:00, 20.89batches/s, epoch=3, loss=6.2, split=eval]    /opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f74693e0610> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 21.28batches/s, epoch=3, loss=6.2, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-300/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-300/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-300/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.075151205062866, 'eval_runtime': 1.5056, 'eval_samples_per_second': 20.59, 'eval_steps_per_second': 20.59, 'eval_SequenceClassification_loss': 1.5520200504324748, 'eval_SequenceClassification_num_batches': 96, 'eval_SequenceClassification_SequenceAucPr': 0.75, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-300/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:02<00:00,  8.44batches/s, epoch=3, loss=0.00304, split=train]Converged objectives: []\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:02<00:00,  7.66batches/s, epoch=3, loss=0.00304, split=train]\n",
      "SequenceClassification:  33%|███▎      | 157/481 [00:18<00:37,  8.53batches/s, epoch=4, loss=0.00361, split=train]Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4741, 'learning_rate': 1.6000000000000003e-05, 'train_SequenceClassification_loss': 1.1794152363205794, 'train_SequenceClassification_num_batches': 1000, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:   0%|          | 0/31 [00:51<?, ?batches/s, epoch=3, loss=-1, split=eval]\n",
      "SequenceClassification: 100%|██████████| 31/31 [00:01<00:00, 20.81batches/s, epoch=4, loss=5.97, split=eval]   /opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f74681c07c0> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 21.06batches/s, epoch=4, loss=5.97, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-400/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-400/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-400/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.960547685623169, 'eval_runtime': 1.5165, 'eval_samples_per_second': 20.442, 'eval_steps_per_second': 20.442, 'eval_SequenceClassification_loss': 1.9276417622950248, 'eval_SequenceClassification_num_batches': 128, 'eval_SequenceClassification_SequenceAucPr': 0.75, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-400/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:02<00:00,  8.45batches/s, epoch=4, loss=0.00125, split=train]Converged objectives: []\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:02<00:00,  7.65batches/s, epoch=4, loss=0.00125, split=train]\n",
      "SequenceClassification:  16%|█▌        | 76/481 [00:09<00:47,  8.50batches/s, epoch=5, loss=0.0661, split=train]Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5271, 'learning_rate': 2e-05, 'train_SequenceClassification_loss': 1.368099317157641, 'train_SequenceClassification_num_batches': 1000, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:   0%|          | 0/31 [00:51<?, ?batches/s, epoch=4, loss=-1, split=eval]\n",
      "SequenceClassification:  97%|█████████▋| 30/31 [00:01<00:00, 20.66batches/s, epoch=5, loss=3.35, split=eval]  /opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f74681c0190> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 21.03batches/s, epoch=5, loss=3.34, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-500/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-500/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-500/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6771562099456787, 'eval_runtime': 1.5242, 'eval_samples_per_second': 20.339, 'eval_steps_per_second': 20.339, 'eval_SequenceClassification_loss': 1.8879416634721564, 'eval_SequenceClassification_num_batches': 160, 'eval_SequenceClassification_SequenceAucPr': 0.75, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-500/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification:  99%|█████████▉| 476/481 [01:02<00:00,  8.52batches/s, epoch=5, loss=0.00125, split=train]Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5656, 'learning_rate': 2.4e-05, 'train_SequenceClassification_loss': 1.1064297522277338, 'train_SequenceClassification_num_batches': 1000, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:   0%|          | 0/31 [00:51<?, ?batches/s, epoch=5, loss=-1, split=eval]\n",
      "SequenceClassification:  97%|█████████▋| 30/31 [00:01<00:00, 20.91batches/s, epoch=5, loss=0.000917, split=eval]/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f746813f610> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 21.31batches/s, epoch=5, loss=0.000917, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-600/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-600/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-600/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.526146650314331, 'eval_runtime': 1.5039, 'eval_samples_per_second': 20.614, 'eval_steps_per_second': 20.614, 'eval_SequenceClassification_loss': 2.14261532723746, 'eval_SequenceClassification_num_batches': 192, 'eval_SequenceClassification_SequenceAucPr': 0.75, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-600/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:07<00:00,  2.07batches/s, epoch=5, loss=0.00116, split=train]Converged objectives: []\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:07<00:00,  7.08batches/s, epoch=5, loss=0.00116, split=train]\n",
      "SequenceClassification:  82%|████████▏ | 395/481 [00:47<00:10,  8.43batches/s, epoch=6, loss=0.00164, split=train]Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3891, 'learning_rate': 2.8000000000000003e-05, 'train_SequenceClassification_loss': 1.0452638140710768, 'train_SequenceClassification_num_batches': 1000, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:   0%|          | 0/31 [00:51<?, ?batches/s, epoch=5, loss=-1, split=eval]\n",
      "SequenceClassification:  97%|█████████▋| 30/31 [00:01<00:00, 20.86batches/s, epoch=6, loss=0.0011, split=eval]/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f746814abe0> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 21.24batches/s, epoch=6, loss=0.0011, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-700/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-700/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-700/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.4335293769836426, 'eval_runtime': 1.5088, 'eval_samples_per_second': 20.546, 'eval_steps_per_second': 20.546, 'eval_SequenceClassification_loss': 2.3117083689742555, 'eval_SequenceClassification_num_batches': 224, 'eval_SequenceClassification_SequenceAucPr': 0.75, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-700/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:02<00:00,  8.28batches/s, epoch=6, loss=0.000859, split=train]Converged objectives: []\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:03<00:00,  7.63batches/s, epoch=6, loss=0.000859, split=train]\n",
      "SequenceClassification:  65%|██████▌   | 314/481 [00:37<00:19,  8.45batches/s, epoch=7, loss=0.079, split=train]   Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5596, 'learning_rate': 3.2000000000000005e-05, 'train_SequenceClassification_loss': 1.2059903278655255, 'train_SequenceClassification_num_batches': 1000, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:   0%|          | 0/31 [00:51<?, ?batches/s, epoch=6, loss=-1, split=eval]\n",
      "SequenceClassification:  97%|█████████▋| 30/31 [00:01<00:00, 20.77batches/s, epoch=7, loss=0.0371, split=eval]/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f74681c0520> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 21.04batches/s, epoch=7, loss=0.0371, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-800/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-800/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-800/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6877338886260986, 'eval_runtime': 1.5229, 'eval_samples_per_second': 20.356, 'eval_steps_per_second': 20.356, 'eval_SequenceClassification_loss': 2.227263622362443, 'eval_SequenceClassification_num_batches': 256, 'eval_SequenceClassification_SequenceAucPr': 0.75, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-800/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:03<00:00,  8.44batches/s, epoch=7, loss=0.00127, split=train]Converged objectives: []\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:03<00:00,  7.62batches/s, epoch=7, loss=0.00127, split=train]\n",
      "SequenceClassification:  48%|████▊     | 233/481 [00:27<00:29,  8.51batches/s, epoch=8, loss=0.000743, split=train]Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7971, 'learning_rate': 3.6e-05, 'train_SequenceClassification_loss': 1.1903262593513937, 'train_SequenceClassification_num_batches': 1000, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:   0%|          | 0/31 [00:51<?, ?batches/s, epoch=7, loss=-1, split=eval]\n",
      "SequenceClassification:  97%|█████████▋| 30/31 [00:01<00:00, 20.77batches/s, epoch=8, loss=7.61, split=eval]    /opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f746813f160> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 21.20batches/s, epoch=8, loss=7.61, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-900/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-900/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-900/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.773026704788208, 'eval_runtime': 1.5114, 'eval_samples_per_second': 20.51, 'eval_steps_per_second': 20.51, 'eval_SequenceClassification_loss': 2.4123271010111136, 'eval_SequenceClassification_num_batches': 288, 'eval_SequenceClassification_SequenceAucPr': 0.75, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-900/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:02<00:00,  8.46batches/s, epoch=8, loss=0.000975, split=train]Converged objectives: []\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:02<00:00,  7.64batches/s, epoch=8, loss=0.000975, split=train]\n",
      "SequenceClassification:  32%|███▏      | 152/481 [00:18<00:38,  8.52batches/s, epoch=9, loss=0.00243, split=train]Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4882, 'learning_rate': 4e-05, 'train_SequenceClassification_loss': 1.2306209717416206, 'train_SequenceClassification_num_batches': 1000, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:   0%|          | 0/31 [00:51<?, ?batches/s, epoch=8, loss=-1, split=eval]\n",
      "SequenceClassification:  97%|█████████▋| 30/31 [00:01<00:00, 20.76batches/s, epoch=9, loss=6.44, split=eval]   /opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f74693e0190> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 21.11batches/s, epoch=9, loss=6.44, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-1000/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-1000/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-1000/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.196300983428955, 'eval_runtime': 1.5185, 'eval_samples_per_second': 20.415, 'eval_steps_per_second': 20.415, 'eval_SequenceClassification_loss': 2.5008701537619347, 'eval_SequenceClassification_num_batches': 320, 'eval_SequenceClassification_SequenceAucPr': 0.75, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-1000/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:03<00:00,  8.19batches/s, epoch=9, loss=0.000923, split=train]Converged objectives: []\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:03<00:00,  7.57batches/s, epoch=9, loss=0.000923, split=train]\n",
      "SequenceClassification:  15%|█▍        | 71/481 [00:08<00:48,  8.49batches/s, epoch=10, loss=0.0869, split=train]Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5527, 'learning_rate': 4.4000000000000006e-05, 'train_SequenceClassification_loss': 1.331231181347277, 'train_SequenceClassification_num_batches': 1000, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:   0%|          | 0/31 [00:52<?, ?batches/s, epoch=9, loss=-1, split=eval]\n",
      "SequenceClassification:  97%|█████████▋| 30/31 [00:01<00:00, 20.70batches/s, epoch=10, loss=3.09, split=eval]  /opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f74801aa4f0> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 21.11batches/s, epoch=10, loss=3.09, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-1100/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-1100/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-1100/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5540165901184082, 'eval_runtime': 1.5182, 'eval_samples_per_second': 20.419, 'eval_steps_per_second': 20.419, 'eval_SequenceClassification_loss': 2.419144620272246, 'eval_SequenceClassification_num_batches': 352, 'eval_SequenceClassification_SequenceAucPr': 0.75, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-1100/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification:  98%|█████████▊| 471/481 [01:01<00:01,  8.51batches/s, epoch=10, loss=0.00141, split=train]Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7692, 'learning_rate': 4.8e-05, 'train_SequenceClassification_loss': 1.2521414867267013, 'train_SequenceClassification_num_batches': 1000, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:   0%|          | 0/31 [00:51<?, ?batches/s, epoch=10, loss=-1, split=eval]\n",
      "SequenceClassification: 100%|██████████| 31/31 [00:01<00:00, 20.64batches/s, epoch=10, loss=0.00105, split=eval]/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f74681b3b20> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 21.03batches/s, epoch=10, loss=0.00105, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-1200/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-1200/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-1200/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.460289478302002, 'eval_runtime': 1.524, 'eval_samples_per_second': 20.341, 'eval_steps_per_second': 20.341, 'eval_SequenceClassification_loss': 2.496898247032732, 'eval_SequenceClassification_num_batches': 384, 'eval_SequenceClassification_SequenceAucPr': 0.75, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-1200/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:08<00:00,  5.43batches/s, epoch=10, loss=0.00134, split=train]Converged objectives: []\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:08<00:00,  7.03batches/s, epoch=10, loss=0.00134, split=train]\n",
      "SequenceClassification:  49%|████▉     | 237/481 [00:28<00:28,  8.46batches/s, epoch=11, loss=0.00137, split=train]Model weights saved in dnabert_for_clash/checkpoint-1400/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:02<00:00,  8.50batches/s, epoch=12, loss=0.00171, split=train]Converged objectives: []\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:02<00:00,  7.64batches/s, epoch=12, loss=0.00171, split=train]\n",
      "SequenceClassification:  47%|████▋     | 228/481 [00:27<00:29,  8.45batches/s, epoch=13, loss=0.00223, split=train]Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9945, 'learning_rate': 6e-05, 'train_SequenceClassification_loss': 1.3155318958439166, 'train_SequenceClassification_num_batches': 1000, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:   0%|          | 0/31 [00:51<?, ?batches/s, epoch=12, loss=-1, split=eval]\n",
      "SequenceClassification:  94%|█████████▎| 29/31 [00:01<00:00, 20.81batches/s, epoch=13, loss=6.36, split=eval]   /opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f74681b3d00> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 20.99batches/s, epoch=13, loss=6.36, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-1500/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-1500/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-1500/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.1574337482452393, 'eval_runtime': 1.5276, 'eval_samples_per_second': 20.293, 'eval_steps_per_second': 20.293, 'eval_SequenceClassification_loss': 2.5026376287671157, 'eval_SequenceClassification_num_batches': 480, 'eval_SequenceClassification_SequenceAucPr': 0.75, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-1500/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:02<00:00,  8.16batches/s, epoch=13, loss=0.00254, split=train]Converged objectives: []\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:02<00:00,  7.65batches/s, epoch=13, loss=0.00254, split=train]\n",
      "SequenceClassification:  31%|███       | 147/481 [00:17<00:39,  8.43batches/s, epoch=14, loss=0.0318, split=train]Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.164, 'learning_rate': 6.400000000000001e-05, 'train_SequenceClassification_loss': 1.5507558739959495, 'train_SequenceClassification_num_batches': 1000, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:   0%|          | 0/31 [00:51<?, ?batches/s, epoch=13, loss=-1, split=eval]\n",
      "SequenceClassification:  97%|█████████▋| 30/31 [00:01<00:00, 20.82batches/s, epoch=14, loss=3.77, split=eval]  /opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f74693b6b50> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 21.19batches/s, epoch=14, loss=3.77, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-1600/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-1600/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-1600/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8802082538604736, 'eval_runtime': 1.5127, 'eval_samples_per_second': 20.493, 'eval_steps_per_second': 20.493, 'eval_SequenceClassification_loss': 2.4674213787498047, 'eval_SequenceClassification_num_batches': 512, 'eval_SequenceClassification_SequenceAucPr': 0.75, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-1600/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:02<00:00,  8.26batches/s, epoch=14, loss=0.00551, split=train]Converged objectives: []\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:02<00:00,  7.66batches/s, epoch=14, loss=0.00551, split=train]\n",
      "SequenceClassification:  14%|█▎        | 66/481 [00:07<00:49,  8.43batches/s, epoch=15, loss=4.02, split=train]Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.078, 'learning_rate': 6.800000000000001e-05, 'train_SequenceClassification_loss': 1.9187223217766731, 'train_SequenceClassification_num_batches': 1000, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:   0%|          | 0/31 [00:51<?, ?batches/s, epoch=14, loss=-1, split=eval]\n",
      "SequenceClassification:  94%|█████████▎| 29/31 [00:01<00:00, 20.78batches/s, epoch=15, loss=0.0185, split=eval]/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f74680a5400> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 21.07batches/s, epoch=15, loss=0.0185, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-1700/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-1700/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-1700/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0256054401397705, 'eval_runtime': 1.5212, 'eval_samples_per_second': 20.378, 'eval_steps_per_second': 20.378, 'eval_SequenceClassification_loss': 2.437742617353198, 'eval_SequenceClassification_num_batches': 544, 'eval_SequenceClassification_SequenceAucPr': 0.75, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-1700/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification:  97%|█████████▋| 466/481 [01:01<00:01,  8.38batches/s, epoch=15, loss=0.00721, split=train]Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3601, 'learning_rate': 7.2e-05, 'train_SequenceClassification_loss': 1.808089804451447, 'train_SequenceClassification_num_batches': 1000, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:   0%|          | 0/31 [00:51<?, ?batches/s, epoch=15, loss=-1, split=eval]\n",
      "SequenceClassification:  94%|█████████▎| 29/31 [00:01<00:00, 20.90batches/s, epoch=15, loss=0.00594, split=eval]/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f746809d820> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 21.12batches/s, epoch=15, loss=0.00594, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-1800/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-1800/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-1800/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.588226318359375, 'eval_runtime': 1.5174, 'eval_samples_per_second': 20.429, 'eval_steps_per_second': 20.429, 'eval_SequenceClassification_loss': 2.4416197272564912, 'eval_SequenceClassification_num_batches': 576, 'eval_SequenceClassification_SequenceAucPr': 0.75, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-1800/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:08<00:00,  7.66batches/s, epoch=15, loss=0.00551, split=train]Converged objectives: []\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:08<00:00,  7.06batches/s, epoch=15, loss=0.00551, split=train]\n",
      "SequenceClassification:  80%|████████  | 385/481 [00:46<00:16,  5.93batches/s, epoch=16, loss=0.0339, split=train]Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0193, 'learning_rate': 7.6e-05, 'train_SequenceClassification_loss': 1.716939680948388, 'train_SequenceClassification_num_batches': 1000, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:   0%|          | 0/31 [00:51<?, ?batches/s, epoch=15, loss=-1, split=eval]\n",
      "SequenceClassification: 100%|██████████| 31/31 [00:01<00:00, 20.47batches/s, epoch=16, loss=0.0283, split=eval]/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f74801786d0> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 18.82batches/s, epoch=16, loss=0.0283, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-1900/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-1900/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-1900/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8177731037139893, 'eval_runtime': 1.7027, 'eval_samples_per_second': 18.207, 'eval_steps_per_second': 18.207, 'eval_SequenceClassification_loss': 2.4058424976664305, 'eval_SequenceClassification_num_batches': 608, 'eval_SequenceClassification_SequenceAucPr': 0.75, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-1900/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:03<00:00,  8.48batches/s, epoch=16, loss=0.00503, split=train]Converged objectives: []\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:03<00:00,  7.62batches/s, epoch=16, loss=0.00503, split=train]\n",
      "SequenceClassification:  63%|██████▎   | 304/481 [00:36<00:20,  8.47batches/s, epoch=17, loss=3.77, split=train]  Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9698, 'learning_rate': 8e-05, 'train_SequenceClassification_loss': 1.8812650823076256, 'train_SequenceClassification_num_batches': 1000, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:   0%|          | 0/31 [00:51<?, ?batches/s, epoch=16, loss=-1, split=eval]\n",
      "SequenceClassification:  97%|█████████▋| 30/31 [00:01<00:00, 20.54batches/s, epoch=17, loss=3.72, split=eval]  /opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f746819d7c0> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 21.07batches/s, epoch=17, loss=3.72, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-2000/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-2000/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-2000/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.857690453529358, 'eval_runtime': 1.5209, 'eval_samples_per_second': 20.382, 'eval_steps_per_second': 20.382, 'eval_SequenceClassification_loss': 2.3813458556953266, 'eval_SequenceClassification_num_batches': 640, 'eval_SequenceClassification_SequenceAucPr': 0.75, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-2000/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification:  46%|████▋     | 223/481 [00:26<00:30,  8.53batches/s, epoch=18, loss=0.0981, split=train]Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6324, 'learning_rate': 8.4e-05, 'train_SequenceClassification_loss': 1.7612629435588605, 'train_SequenceClassification_num_batches': 1000, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:   0%|          | 0/31 [00:51<?, ?batches/s, epoch=17, loss=-1, split=eval]\n",
      "SequenceClassification:  97%|█████████▋| 30/31 [00:01<00:00, 20.82batches/s, epoch=18, loss=2.53, split=eval]  /opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f74693cff10> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 21.25batches/s, epoch=18, loss=2.53, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-2100/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-2100/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-2100/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2974705696105957, 'eval_runtime': 1.508, 'eval_samples_per_second': 20.557, 'eval_steps_per_second': 20.557, 'eval_SequenceClassification_loss': 2.331569596953549, 'eval_SequenceClassification_num_batches': 672, 'eval_SequenceClassification_SequenceAucPr': 0.75, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-2100/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:02<00:00,  8.21batches/s, epoch=18, loss=0.00453, split=train]Converged objectives: []\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:02<00:00,  7.64batches/s, epoch=18, loss=0.00453, split=train]\n",
      "SequenceClassification:  30%|██▉       | 142/481 [00:17<00:40,  8.45batches/s, epoch=19, loss=1.91, split=train]Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7434, 'learning_rate': 8.800000000000001e-05, 'train_SequenceClassification_loss': 1.6686349766086788, 'train_SequenceClassification_num_batches': 1000, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:   0%|          | 0/31 [00:52<?, ?batches/s, epoch=18, loss=-1, split=eval]\n",
      "SequenceClassification:  94%|█████████▎| 29/31 [00:01<00:00, 20.67batches/s, epoch=19, loss=0.183, split=eval]/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f746815daf0> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 21.13batches/s, epoch=19, loss=0.183, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-2200/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-2200/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-2200/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9925918579101562, 'eval_runtime': 1.5165, 'eval_samples_per_second': 20.442, 'eval_steps_per_second': 20.442, 'eval_SequenceClassification_loss': 2.2695566748579123, 'eval_SequenceClassification_num_batches': 704, 'eval_SequenceClassification_SequenceAucPr': 0.75, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-2200/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:03<00:00,  8.37batches/s, epoch=19, loss=0.00493, split=train]Converged objectives: []\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:03<00:00,  7.60batches/s, epoch=19, loss=0.00493, split=train]\n",
      "SequenceClassification:  13%|█▎        | 61/481 [00:07<00:49,  8.42batches/s, epoch=20, loss=4.22, split=train]Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.624, 'learning_rate': 9.200000000000001e-05, 'train_SequenceClassification_loss': 1.8100189054151996, 'train_SequenceClassification_num_batches': 1000, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:   0%|          | 0/31 [00:51<?, ?batches/s, epoch=19, loss=-1, split=eval]\n",
      "SequenceClassification:  94%|█████████▎| 29/31 [00:01<00:00, 20.58batches/s, epoch=20, loss=0.0163, split=eval]/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f74802f6a90> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 20.97batches/s, epoch=20, loss=0.0163, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-2300/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-2300/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-2300/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.087830066680908, 'eval_runtime': 1.5283, 'eval_samples_per_second': 20.284, 'eval_steps_per_second': 20.284, 'eval_SequenceClassification_loss': 2.2588409181768405, 'eval_SequenceClassification_num_batches': 736, 'eval_SequenceClassification_SequenceAucPr': 0.75, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-2300/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification:  96%|█████████▌| 461/481 [01:00<00:02,  8.54batches/s, epoch=20, loss=0.0101, split=train]Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3914, 'learning_rate': 9.6e-05, 'train_SequenceClassification_loss': 1.771668959448114, 'train_SequenceClassification_num_batches': 1000, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:   0%|          | 0/31 [00:51<?, ?batches/s, epoch=20, loss=-1, split=eval]\n",
      "SequenceClassification:  97%|█████████▋| 30/31 [00:01<00:00, 20.80batches/s, epoch=20, loss=0.0083, split=eval]/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f74693e02b0> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 21.10batches/s, epoch=20, loss=0.0083, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-2400/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-2400/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-2400/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.421452283859253, 'eval_runtime': 1.5192, 'eval_samples_per_second': 20.405, 'eval_steps_per_second': 20.405, 'eval_SequenceClassification_loss': 2.262474231281582, 'eval_SequenceClassification_num_batches': 768, 'eval_SequenceClassification_SequenceAucPr': 0.75, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-2400/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:07<00:00,  8.43batches/s, epoch=20, loss=0.00594, split=train]Converged objectives: []\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:07<00:00,  7.08batches/s, epoch=20, loss=0.00594, split=train]\n",
      "SequenceClassification:  79%|███████▉  | 380/481 [00:45<00:11,  8.48batches/s, epoch=21, loss=0.98, split=train]  Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3961, 'learning_rate': 0.0001, 'train_SequenceClassification_loss': 1.8422518291380257, 'train_SequenceClassification_num_batches': 1000, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:   0%|          | 0/31 [00:51<?, ?batches/s, epoch=20, loss=-1, split=eval]\n",
      "SequenceClassification:  97%|█████████▋| 30/31 [00:01<00:00, 20.86batches/s, epoch=21, loss=0.887, split=eval]/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f74693cfd30> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 21.13batches/s, epoch=21, loss=0.887, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-2500/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-2500/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-2500/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7075479030609131, 'eval_runtime': 1.5166, 'eval_samples_per_second': 20.44, 'eval_steps_per_second': 20.44, 'eval_SequenceClassification_loss': 2.2005020512783084, 'eval_SequenceClassification_num_batches': 800, 'eval_SequenceClassification_SequenceAucPr': 0.75, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-2500/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:02<00:00,  8.23batches/s, epoch=21, loss=0.0333, split=train]Converged objectives: []\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:02<00:00,  7.66batches/s, epoch=21, loss=0.0333, split=train]\n",
      "SequenceClassification:  62%|██████▏   | 299/481 [00:35<00:21,  8.53batches/s, epoch=22, loss=3.46, split=train]  Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.646, 'learning_rate': 0.00010400000000000001, 'train_SequenceClassification_loss': 1.8872897774269803, 'train_SequenceClassification_num_batches': 1000, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:   0%|          | 0/31 [00:51<?, ?batches/s, epoch=21, loss=-1, split=eval]\n",
      "SequenceClassification:  97%|█████████▋| 30/31 [00:01<00:00, 20.92batches/s, epoch=22, loss=3.41, split=eval]  /opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f74680d1850> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 21.32batches/s, epoch=22, loss=3.41, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-2600/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-2600/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-2600/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7061891555786133, 'eval_runtime': 1.5035, 'eval_samples_per_second': 20.619, 'eval_steps_per_second': 20.619, 'eval_SequenceClassification_loss': 2.183532863358638, 'eval_SequenceClassification_num_batches': 832, 'eval_SequenceClassification_SequenceAucPr': 0.75, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-2600/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:02<00:00,  8.36batches/s, epoch=22, loss=0.0199, split=train]Converged objectives: []\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:02<00:00,  7.65batches/s, epoch=22, loss=0.0199, split=train]\n",
      "SequenceClassification:  45%|████▌     | 218/481 [00:26<00:30,  8.50batches/s, epoch=23, loss=0.129, split=train]Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6293, 'learning_rate': 0.00010800000000000001, 'train_SequenceClassification_loss': 1.7314232522919775, 'train_SequenceClassification_num_batches': 1000, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:   0%|          | 0/31 [00:51<?, ?batches/s, epoch=22, loss=-1, split=eval]\n",
      "SequenceClassification:  97%|█████████▋| 30/31 [00:01<00:00, 20.83batches/s, epoch=23, loss=2.29, split=eval] /opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f746815df40> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 21.19batches/s, epoch=23, loss=2.29, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-2700/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-2700/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-2700/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1881483793258667, 'eval_runtime': 1.5124, 'eval_samples_per_second': 20.497, 'eval_steps_per_second': 20.497, 'eval_SequenceClassification_loss': 2.147938285838151, 'eval_SequenceClassification_num_batches': 864, 'eval_SequenceClassification_SequenceAucPr': 0.75, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-2700/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:02<00:00,  8.42batches/s, epoch=23, loss=0.0099, split=train] Converged objectives: []\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:02<00:00,  7.69batches/s, epoch=23, loss=0.0099, split=train]\n",
      "SequenceClassification:  28%|██▊       | 137/481 [00:16<00:40,  8.39batches/s, epoch=24, loss=0.226, split=train]Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8798, 'learning_rate': 0.00011200000000000001, 'train_SequenceClassification_loss': 1.7134413984753192, 'train_SequenceClassification_num_batches': 1000, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:   0%|          | 0/31 [00:51<?, ?batches/s, epoch=23, loss=-1, split=eval]\n",
      "SequenceClassification:  94%|█████████▎| 29/31 [00:01<00:00, 20.75batches/s, epoch=24, loss=1.72, split=eval] /opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f74681292e0> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 21.09batches/s, epoch=24, loss=1.72, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-2800/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-2800/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-2800/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9526139497756958, 'eval_runtime': 1.5191, 'eval_samples_per_second': 20.407, 'eval_steps_per_second': 20.407, 'eval_SequenceClassification_loss': 2.106104872726064, 'eval_SequenceClassification_num_batches': 896, 'eval_SequenceClassification_SequenceAucPr': 0.75, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-2800/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:03<00:00,  8.44batches/s, epoch=24, loss=0.0624, split=train]Converged objectives: []\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:03<00:00,  7.62batches/s, epoch=24, loss=0.0624, split=train]\n",
      "SequenceClassification:  12%|█▏        | 56/481 [00:06<00:49,  8.51batches/s, epoch=25, loss=2.97, split=train]Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7686, 'learning_rate': 0.000116, 'train_SequenceClassification_loss': 1.8605698600951581, 'train_SequenceClassification_num_batches': 1000, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:   0%|          | 0/31 [00:51<?, ?batches/s, epoch=24, loss=-1, split=eval]\n",
      "SequenceClassification:  94%|█████████▎| 29/31 [00:01<00:00, 20.84batches/s, epoch=25, loss=0.0553, split=eval]/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f74680a36d0> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 21.05batches/s, epoch=25, loss=0.0553, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-2900/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-2900/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-2900/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5007637739181519, 'eval_runtime': 1.5222, 'eval_samples_per_second': 20.365, 'eval_steps_per_second': 20.365, 'eval_SequenceClassification_loss': 2.083673400286804, 'eval_SequenceClassification_num_batches': 928, 'eval_SequenceClassification_SequenceAucPr': 0.75, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-2900/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification:  95%|█████████▍| 456/481 [01:00<00:02,  8.45batches/s, epoch=25, loss=0.138, split=train] Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5643, 'learning_rate': 0.00012, 'train_SequenceClassification_loss': 1.7857373125180602, 'train_SequenceClassification_num_batches': 1000, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:   0%|          | 0/31 [00:51<?, ?batches/s, epoch=25, loss=-1, split=eval]\n",
      "SequenceClassification:  97%|█████████▋| 30/31 [00:01<00:00, 20.79batches/s, epoch=25, loss=0.112, split=eval]/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f74693e0af0> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 21.18batches/s, epoch=25, loss=0.112, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-3000/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-3000/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-3000/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1867918968200684, 'eval_runtime': 1.5245, 'eval_samples_per_second': 20.334, 'eval_steps_per_second': 20.334, 'eval_SequenceClassification_loss': 2.0526578336064025, 'eval_SequenceClassification_num_batches': 960, 'eval_SequenceClassification_SequenceAucPr': 0.75, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-3000/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:08<00:00,  8.11batches/s, epoch=25, loss=0.0427, split=train]Converged objectives: []\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:08<00:00,  7.06batches/s, epoch=25, loss=0.0427, split=train]\n",
      "SequenceClassification:  78%|███████▊  | 375/481 [00:45<00:12,  8.47batches/s, epoch=26, loss=1.25, split=train]  Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9925, 'learning_rate': 0.000124, 'train_SequenceClassification_loss': 1.7378839739114047, 'train_SequenceClassification_num_batches': 1000, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:   0%|          | 0/31 [00:51<?, ?batches/s, epoch=25, loss=-1, split=eval]\n",
      "SequenceClassification:  94%|█████████▎| 29/31 [00:01<00:00, 20.74batches/s, epoch=26, loss=1.16, split=eval] /opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f7469316ee0> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 21.08batches/s, epoch=26, loss=1.16, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-3100/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-3100/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-3100/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7635958194732666, 'eval_runtime': 1.5201, 'eval_samples_per_second': 20.394, 'eval_steps_per_second': 20.394, 'eval_SequenceClassification_loss': 2.011469815621357, 'eval_SequenceClassification_num_batches': 992, 'eval_SequenceClassification_SequenceAucPr': 0.75, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-3100/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:03<00:00,  8.36batches/s, epoch=26, loss=0.033, split=train] Converged objectives: []\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:03<00:00,  7.62batches/s, epoch=26, loss=0.033, split=train]\n",
      "SequenceClassification:  61%|██████    | 294/481 [00:35<00:21,  8.57batches/s, epoch=27, loss=4.06, split=train]  Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6097, 'learning_rate': 0.00012800000000000002, 'train_SequenceClassification_loss': 1.8412406617822126, 'train_SequenceClassification_num_batches': 1000, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:   0%|          | 0/31 [00:51<?, ?batches/s, epoch=26, loss=-1, split=eval]\n",
      "SequenceClassification:  97%|█████████▋| 30/31 [00:01<00:00, 20.89batches/s, epoch=27, loss=3.98, split=eval]  /opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f7469316f10> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 21.33batches/s, epoch=27, loss=3.98, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-3200/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-3200/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-3200/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9823851585388184, 'eval_runtime': 1.5023, 'eval_samples_per_second': 20.635, 'eval_steps_per_second': 20.635, 'eval_SequenceClassification_loss': 2.012509567623283, 'eval_SequenceClassification_num_batches': 1024, 'eval_SequenceClassification_SequenceAucPr': 0.75, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-3200/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:02<00:00,  8.48batches/s, epoch=27, loss=0.0109, split=train]Converged objectives: []\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:02<00:00,  7.65batches/s, epoch=27, loss=0.0109, split=train]\n",
      "SequenceClassification:  44%|████▍     | 213/481 [00:25<00:31,  8.53batches/s, epoch=28, loss=0.0487, split=train]Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6903, 'learning_rate': 0.000132, 'train_SequenceClassification_loss': 1.7600788341006264, 'train_SequenceClassification_num_batches': 1000, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:   0%|          | 0/31 [00:51<?, ?batches/s, epoch=27, loss=-1, split=eval]\n",
      "SequenceClassification:  97%|█████████▋| 30/31 [00:01<00:00, 20.78batches/s, epoch=28, loss=3.27, split=eval]  /opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f746807db20> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 21.23batches/s, epoch=28, loss=3.27, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-3300/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-3300/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-3300/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.639068603515625, 'eval_runtime': 1.5095, 'eval_samples_per_second': 20.536, 'eval_steps_per_second': 20.536, 'eval_SequenceClassification_loss': 2.0027330867475635, 'eval_SequenceClassification_num_batches': 1056, 'eval_SequenceClassification_SequenceAucPr': 0.75, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-3300/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:02<00:00,  8.56batches/s, epoch=28, loss=0.0177, split=train]Converged objectives: []\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:02<00:00,  7.67batches/s, epoch=28, loss=0.0177, split=train]\n",
      "SequenceClassification:  27%|██▋       | 132/481 [00:15<00:40,  8.51batches/s, epoch=29, loss=1.35, split=train]Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2995, 'learning_rate': 0.00013600000000000003, 'train_SequenceClassification_loss': 1.9034341648817061, 'train_SequenceClassification_num_batches': 1000, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:   0%|          | 0/31 [00:51<?, ?batches/s, epoch=28, loss=-1, split=eval]\n",
      "SequenceClassification:  97%|█████████▋| 30/31 [00:01<00:00, 13.63batches/s, epoch=29, loss=0.346, split=eval]/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f74680b3520> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 17.34batches/s, epoch=29, loss=0.346, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-3400/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-3400/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-3400/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7912241220474243, 'eval_runtime': 1.8483, 'eval_samples_per_second': 16.772, 'eval_steps_per_second': 16.772, 'eval_SequenceClassification_loss': 1.9666912605036273, 'eval_SequenceClassification_num_batches': 1088, 'eval_SequenceClassification_SequenceAucPr': 0.75, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-3400/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:02<00:00,  8.17batches/s, epoch=29, loss=0.00892, split=train]Converged objectives: []\n",
      "SequenceClassification: 100%|██████████| 481/481 [01:02<00:00,  7.64batches/s, epoch=29, loss=0.00892, split=train]\n",
      "SequenceClassification:  11%|█         | 51/481 [00:06<00:50,  8.46batches/s, epoch=30, loss=4.4, split=train] Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7679, 'learning_rate': 0.00014, 'train_SequenceClassification_loss': 2.03755366403237, 'train_SequenceClassification_num_batches': 1000, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:   0%|          | 0/31 [00:51<?, ?batches/s, epoch=29, loss=-1, split=eval]\n",
      "SequenceClassification:  94%|█████████▎| 29/31 [00:01<00:00, 20.77batches/s, epoch=30, loss=0.0132, split=eval]/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f7469316a00> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 21.20batches/s, epoch=30, loss=0.0132, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-3500/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-3500/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-3500/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1921253204345703, 'eval_runtime': 1.5118, 'eval_samples_per_second': 20.505, 'eval_steps_per_second': 20.505, 'eval_SequenceClassification_loss': 1.9711867190650083, 'eval_SequenceClassification_num_batches': 1120, 'eval_SequenceClassification_SequenceAucPr': 0.75, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-3500/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification:  94%|█████████▍| 451/481 [00:58<00:03,  8.52batches/s, epoch=30, loss=0.0189, split=train]Converged objectives: []\n",
      "Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6123, 'learning_rate': 0.000144, 'train_SequenceClassification_loss': 1.8092812393018975, 'train_SequenceClassification_num_batches': 1000, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SequenceClassification:   0%|          | 0/31 [00:51<?, ?batches/s, epoch=30, loss=-1, split=eval]\n",
      "SequenceClassification:  97%|█████████▋| 30/31 [00:01<00:00, 20.78batches/s, epoch=30, loss=0.0149, split=eval]/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:542: UserWarning: Length of IterableDataset <adaptor.utils.TransformerAdaptationDataset object at 0x7f746818bb20> was reported to be 31 (when accessing len(dataloader)), but 32 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "SequenceClassification: 32batches [00:01, 21.21batches/s, epoch=30, loss=0.0149, split=eval]                   \n",
      "Converged objectives: []\n",
      "tokenizer config file saved in dnabert_for_clash/checkpoint-3600/SequenceClassification/tokenizer_config.json\n",
      "Special tokens file saved in dnabert_for_clash/checkpoint-3600/SequenceClassification/special_tokens_map.json\n",
      "Configuration saved in dnabert_for_clash/checkpoint-3600/SequenceClassification/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1321206092834473, 'eval_runtime': 1.5146, 'eval_samples_per_second': 20.468, 'eval_steps_per_second': 20.468, 'eval_SequenceClassification_loss': 1.973819205167375, 'eval_SequenceClassification_num_batches': 1152, 'eval_SequenceClassification_SequenceAucPr': 0.75, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dnabert_for_clash/checkpoint-3600/SequenceClassification/pytorch_model.bin\n",
      "SequenceClassification:  97%|█████████▋| 466/481 [01:05<00:01,  7.78batches/s, epoch=30, loss=0.00971, split=train]"
     ]
    }
   ],
   "source": [
    "# 4. Run the training using Adapter, similarly to running HF.Trainer, only adding `schedule`\n",
    "from adaptor.adapter import Adapter\n",
    "#from transformers.integrations import CometCallback\n",
    "\n",
    "adapter = Adapter(lang_module=lang_module, schedule=schedule, args=training_arguments)#, callbacks=[CometCallback()])\n",
    "adapter.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPY27lPqYCuXHHH6okv90GH",
   "include_colab_link": true,
   "name": "Untitled9.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
